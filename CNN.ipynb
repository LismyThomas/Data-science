{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215a438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 13:37:28.207053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 13:37:28.207086: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c02d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "(X_train,Y_train),(X_valid,Y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522f28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "X_train =X_train .reshape(60000,784).astype('float32')\n",
    "X_valid =X_valid .reshape(10000,784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd6bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /=255\n",
    "X_valid /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e784448d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
       "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
       "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
       "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
       "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
       "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
       "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
       "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
       "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
       "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056cd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the labels to one hot representation.\n",
    "from keras import utils as np_utils\n",
    "n_classes=10\n",
    "Y_train=keras.utils.np_utils.to_categorical(Y_train,n_classes)\n",
    "Y_valid=keras.utils.np_utils.to_categorical(Y_valid,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353c8a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281f4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 13:43:16.789742: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-07 13:43:16.789782: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-07 13:43:16.789809: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterhub): /proc/driver/nvidia/version does not exist\n",
      "2022-11-07 13:43:16.811739: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba37752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64,activation='sigmoid',input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6d64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the final layer\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ec7347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09125ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the network\n",
    "model.compile(loss='mean_squared_error',optimizer=SGD(learning_rate=0.01),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64ab9d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 13:44:25.183636: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "469/469 [==============================] - 2s 2ms/step - loss: 0.0916 - accuracy: 0.0942\n",
      "Epoch 2/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0909 - accuracy: 0.1163\n",
      "Epoch 3/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0903 - accuracy: 0.1337\n",
      "Epoch 4/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0899 - accuracy: 0.1497\n",
      "Epoch 5/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0895 - accuracy: 0.1709\n",
      "Epoch 6/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0892 - accuracy: 0.1960\n",
      "Epoch 7/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0889 - accuracy: 0.2193\n",
      "Epoch 8/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0886 - accuracy: 0.2412\n",
      "Epoch 9/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0883 - accuracy: 0.2693\n",
      "Epoch 10/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0880 - accuracy: 0.2988\n",
      "Epoch 11/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0878 - accuracy: 0.3289\n",
      "Epoch 12/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0875 - accuracy: 0.3582\n",
      "Epoch 13/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0873 - accuracy: 0.3879\n",
      "Epoch 14/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0870 - accuracy: 0.4173\n",
      "Epoch 15/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0868 - accuracy: 0.4381\n",
      "Epoch 16/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0865 - accuracy: 0.4513\n",
      "Epoch 17/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0862 - accuracy: 0.4612\n",
      "Epoch 18/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0860 - accuracy: 0.4650\n",
      "Epoch 19/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0857 - accuracy: 0.4695\n",
      "Epoch 20/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0854 - accuracy: 0.4724\n",
      "Epoch 21/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0851 - accuracy: 0.4722\n",
      "Epoch 22/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0848 - accuracy: 0.4737\n",
      "Epoch 23/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0845 - accuracy: 0.4737\n",
      "Epoch 24/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0842 - accuracy: 0.4739\n",
      "Epoch 25/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0839 - accuracy: 0.4748\n",
      "Epoch 26/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0836 - accuracy: 0.4735\n",
      "Epoch 27/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.4749\n",
      "Epoch 28/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0829 - accuracy: 0.4726\n",
      "Epoch 29/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0825 - accuracy: 0.4744\n",
      "Epoch 30/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0822 - accuracy: 0.4744\n",
      "Epoch 31/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0818 - accuracy: 0.4748\n",
      "Epoch 32/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0814 - accuracy: 0.4742\n",
      "Epoch 33/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0810 - accuracy: 0.4770\n",
      "Epoch 34/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0806 - accuracy: 0.4754\n",
      "Epoch 35/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0802 - accuracy: 0.4768\n",
      "Epoch 36/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0798 - accuracy: 0.4772\n",
      "Epoch 37/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.4780\n",
      "Epoch 38/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0789 - accuracy: 0.4813\n",
      "Epoch 39/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0785 - accuracy: 0.4836\n",
      "Epoch 40/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0780 - accuracy: 0.4844\n",
      "Epoch 41/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0776 - accuracy: 0.4895\n",
      "Epoch 42/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0771 - accuracy: 0.4913\n",
      "Epoch 43/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0766 - accuracy: 0.4943\n",
      "Epoch 44/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0761 - accuracy: 0.4989\n",
      "Epoch 45/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.5028\n",
      "Epoch 46/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0751 - accuracy: 0.5062\n",
      "Epoch 47/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.5123\n",
      "Epoch 48/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0741 - accuracy: 0.5157\n",
      "Epoch 49/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0736 - accuracy: 0.5206\n",
      "Epoch 50/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0731 - accuracy: 0.5263\n",
      "Epoch 51/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0726 - accuracy: 0.5310\n",
      "Epoch 52/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0721 - accuracy: 0.5373\n",
      "Epoch 53/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0716 - accuracy: 0.5440\n",
      "Epoch 54/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0711 - accuracy: 0.5494\n",
      "Epoch 55/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0705 - accuracy: 0.5556\n",
      "Epoch 56/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0700 - accuracy: 0.5612\n",
      "Epoch 57/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0695 - accuracy: 0.5667\n",
      "Epoch 58/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0690 - accuracy: 0.5725\n",
      "Epoch 59/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0684 - accuracy: 0.5799\n",
      "Epoch 60/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0679 - accuracy: 0.5854\n",
      "Epoch 61/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0674 - accuracy: 0.5905\n",
      "Epoch 62/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0669 - accuracy: 0.5972\n",
      "Epoch 63/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0663 - accuracy: 0.6035\n",
      "Epoch 64/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0658 - accuracy: 0.6094\n",
      "Epoch 65/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0653 - accuracy: 0.6146\n",
      "Epoch 66/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0648 - accuracy: 0.6204\n",
      "Epoch 67/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0643 - accuracy: 0.6262\n",
      "Epoch 68/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0637 - accuracy: 0.6320\n",
      "Epoch 69/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0632 - accuracy: 0.6375\n",
      "Epoch 70/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0627 - accuracy: 0.6430\n",
      "Epoch 71/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0622 - accuracy: 0.6487\n",
      "Epoch 72/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0617 - accuracy: 0.6539\n",
      "Epoch 73/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0612 - accuracy: 0.6588\n",
      "Epoch 74/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0607 - accuracy: 0.6643\n",
      "Epoch 75/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0602 - accuracy: 0.6677\n",
      "Epoch 76/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0597 - accuracy: 0.6726\n",
      "Epoch 77/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0593 - accuracy: 0.6768\n",
      "Epoch 78/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0588 - accuracy: 0.6825\n",
      "Epoch 79/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0583 - accuracy: 0.6859\n",
      "Epoch 80/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0578 - accuracy: 0.6905\n",
      "Epoch 81/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0574 - accuracy: 0.6946\n",
      "Epoch 82/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0569 - accuracy: 0.6986\n",
      "Epoch 83/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0564 - accuracy: 0.7022\n",
      "Epoch 84/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0560 - accuracy: 0.7055\n",
      "Epoch 85/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0555 - accuracy: 0.7087\n",
      "Epoch 86/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0551 - accuracy: 0.7119\n",
      "Epoch 87/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0546 - accuracy: 0.7150\n",
      "Epoch 88/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0542 - accuracy: 0.7177\n",
      "Epoch 89/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0538 - accuracy: 0.7209\n",
      "Epoch 90/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0533 - accuracy: 0.7230\n",
      "Epoch 91/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0529 - accuracy: 0.7258\n",
      "Epoch 92/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0525 - accuracy: 0.7281\n",
      "Epoch 93/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0521 - accuracy: 0.7304\n",
      "Epoch 94/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0517 - accuracy: 0.7329\n",
      "Epoch 95/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.7347\n",
      "Epoch 96/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0509 - accuracy: 0.7366\n",
      "Epoch 97/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0505 - accuracy: 0.7385\n",
      "Epoch 98/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0501 - accuracy: 0.7404\n",
      "Epoch 99/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0497 - accuracy: 0.7420\n",
      "Epoch 100/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0493 - accuracy: 0.7439\n",
      "Epoch 101/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0490 - accuracy: 0.7454\n",
      "Epoch 102/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0486 - accuracy: 0.7471\n",
      "Epoch 103/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0482 - accuracy: 0.7484\n",
      "Epoch 104/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0479 - accuracy: 0.7496\n",
      "Epoch 105/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0475 - accuracy: 0.7511\n",
      "Epoch 106/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0472 - accuracy: 0.7524\n",
      "Epoch 107/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0469 - accuracy: 0.7537\n",
      "Epoch 108/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0465 - accuracy: 0.7552\n",
      "Epoch 109/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0462 - accuracy: 0.7563\n",
      "Epoch 110/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0459 - accuracy: 0.7575\n",
      "Epoch 111/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0455 - accuracy: 0.7587\n",
      "Epoch 112/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0452 - accuracy: 0.7597\n",
      "Epoch 113/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0449 - accuracy: 0.7611\n",
      "Epoch 114/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0446 - accuracy: 0.7628\n",
      "Epoch 115/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0443 - accuracy: 0.7640\n",
      "Epoch 116/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0440 - accuracy: 0.7654\n",
      "Epoch 117/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0437 - accuracy: 0.7664\n",
      "Epoch 118/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0434 - accuracy: 0.7680\n",
      "Epoch 119/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0431 - accuracy: 0.7691\n",
      "Epoch 120/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0428 - accuracy: 0.7704\n",
      "Epoch 121/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0425 - accuracy: 0.7723\n",
      "Epoch 122/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.7740\n",
      "Epoch 123/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0420 - accuracy: 0.7757\n",
      "Epoch 124/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0417 - accuracy: 0.7773\n",
      "Epoch 125/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0415 - accuracy: 0.7789\n",
      "Epoch 126/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0412 - accuracy: 0.7811\n",
      "Epoch 127/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0409 - accuracy: 0.7829\n",
      "Epoch 128/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0407 - accuracy: 0.7847\n",
      "Epoch 129/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0404 - accuracy: 0.7869\n",
      "Epoch 130/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0402 - accuracy: 0.7887\n",
      "Epoch 131/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.7906\n",
      "Epoch 132/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0397 - accuracy: 0.7923\n",
      "Epoch 133/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0394 - accuracy: 0.7940\n",
      "Epoch 134/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0392 - accuracy: 0.7955\n",
      "Epoch 135/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0390 - accuracy: 0.7969\n",
      "Epoch 136/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0387 - accuracy: 0.7983\n",
      "Epoch 137/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0385 - accuracy: 0.7998\n",
      "Epoch 138/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0383 - accuracy: 0.8013\n",
      "Epoch 139/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0381 - accuracy: 0.8031\n",
      "Epoch 140/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0378 - accuracy: 0.8044\n",
      "Epoch 141/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0376 - accuracy: 0.8054\n",
      "Epoch 142/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.8070\n",
      "Epoch 143/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0372 - accuracy: 0.8085\n",
      "Epoch 144/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0370 - accuracy: 0.8098\n",
      "Epoch 145/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0368 - accuracy: 0.8109\n",
      "Epoch 146/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0366 - accuracy: 0.8121\n",
      "Epoch 147/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0364 - accuracy: 0.8134\n",
      "Epoch 148/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0362 - accuracy: 0.8145\n",
      "Epoch 149/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0360 - accuracy: 0.8157\n",
      "Epoch 150/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0358 - accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "history=model.fit(X_train,Y_train,batch_size=128,epochs=150,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7e79add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression(random_state=0))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, Y = fetch_openml(data_id=1464, return_X_y=True)\n",
    "X_train, X_valid, Y_train,Y_valid = train_test_split(X, Y, stratify=Y)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8eb8bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "Y_pred = clf.predict(X_valid)\n",
    "cm = confusion_matrix(Y_valid, Y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f844c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
